# EsperanÃ§a matemÃ¡tica

## Index

- [ExtensÃ£o das definiÃ§Ãµes](#extensÃ£o-das-definiÃ§Ãµes)
- [CorrelaÃ§Ã£o](#correlaÃ§Ã£o)
- [IndependÃªncia](#independÃªncia)
- [CovariÃ¢ncia](#covariÃ¢ncia)
- [Coeficiente de correlaÃ§Ã£o](#coeficiente-de-correlaÃ§Ã£o)

## ExtensÃ£o das definiÃ§Ãµes
Os **momentos** de ordem *j* e *k* das variÃ¡veis *X* e *Y* definem-se como sendo:

- Caso **discreto**:
  - E[X^j Y^K]= SUM_m (SUM_n ((x_m)^j * (y_n)^k * p_X,Y(x_m , y_n))) 

- Caso **contÃ­nuo**:
  - E[X^j Y^k] = Integral_-inf to +inf (Integral_-inf to +inf ( x^j * y^k f_XY(x,y) dx) dy)

Se *j=1* e *k=0* ou *j=0* e *k=1* temos os **valores mÃ©dios** de *X* e *Y*

Se *j=2* e *k=0* ou *j=0* e *k=2* temos os **valores quadrÃ¡ticos mÃ©dios**

Os **momentos centrais conjuntos** de ordem *j* e *k* das variÃ¡veis *X* e *Y* definem-se como:
- **E[ (X - E[X])^j (Y - E[Y])^k ]**

Para *j=2* e *k=0* ou *j=0* e *k=2* obtemos as **variÃ¢ncias** de *X* e *Y*

## CorrelaÃ§Ã£o

O momento de ordem *j=k=1*, **E[XY]** Ã© designado de **correlaÃ§Ã£o** das variÃ¡veis *X* e *Y*
- Quando **E[XY]=0** as variÃ¡veis sÃ£o **ortogonais**

## IndependÃªncia

Sendo *X* e *Y* **independentes**
- **E[XY] = E[X] * E[Y]**

**DemonstraÃ§Ã£o:**

E[XY] = SUM_x,y (x * y * p_X,Y(x,y))

= SUM_x,y (x * y * p(x) * p(y))

= [SUM_x (x p_X(x))] * [SUM_x (y p_Y(y))]

= E[X] * E[Y]

## CovariÃ¢ncia

A **covariÃ¢ncia** de duas variÃ¡veis *X* e *Y* Ã© o seu **momento central** de ordem *j=k=1*
- Ou seja **E[(X - E[X]) (Y - E[Y])]**
- Designa-se **Cov(X,Y)**

**DemonstraÃ§Ã£o**

Cov(X,Y) = E[(X - E[X]) * (Y - E[Y])]

= E[X*Y - X*E[Y] - Y*E[X] + E[X]*E[Y]]

= E[XY] - 2*E[X]*E[Y] + E[X]*E[Y]

= E[XY] - E[X]*E[Y]

E[X] = 0 ou E[Y] = 0 => Cov(X,Y) = E[XY]

Ã‰ uma **generalizaÃ§Ã£o da VariÃ¢ncia**
- Cov(X,Y) = E[(X - E[X]) (X - E[X])] = Var(X)

Medida de relaÃ§Ã£o linear entre variÃ¡veiws **aleatÃ³rias**

Se relaÃ§Ã£o **nÃ£o linear**, covariÃ¢ncia pode **nÃ£o ser sensÃ­vel Ã¡ relaÃ§Ã£o**

### CovariÃ¢ncia e independÃªncia

Se *X* e *Y* sÃ£o **independentes*`entÃ£o **Cov(X,Y) = 0**

Uma vez que **Cov(X,Y) = E[XY] - E[X] * E[Y]*, se *X* e *Y* sÃ£o independentes implica:
- **E[XY] = E[X] * E[Y]**
- *O contrarÃ¡rio nÃ£o Ã© verdadeiro, se Cov(X,Y)=0, X e Y podem nÃ£o ser independentes ainda assim*

### Propriedades

1. Cov(X,X) = Var(X)
2. Cov(X,Y) = Cov(Y,X)
3. Cov(cX,Y) = c * Cov(X,Y)
4. Cov(X,Y + Z) = Cov(X,Y) + Cov(X,Z)

**DemonstraÃ§Ã£o 4.**

E[X (Y+Z)] - E[X] * E[Y + Z]

= E[XY] + E[XZ] - E[X]*E[Y] - E[X]*E[Z]

= E[XY] - E[X]*E[Y] + E[XZ] - E[X]*E[Z]

= Cov(X,Y) + Cov (X,Z)

## Coeficiente de correlaÃ§Ã£o

De duas variÃ¡veis, *X* e *Y*:
- **ğœŒ_XY = Cov(X,Y) / (ğœ_X * ğœ_Y)**

Demonstra-se que **-1 <= ğœŒ_XY <= 1**

E que os valores extremos (1 e -1) se obtÃ©m para a relaÃ§Ã£o linear **Y= aX + b* com a>0 ou a<0, respectivamente

Se **ğœŒ_XY = 0** as variÃ¡veis dizem-se **descorrelacionadas** 

**Independentes => descorrelacionadas**

### Exemplo

|x	|y		|P(x,Y)	|
|-	|-		|-		|
|0	|0		|0.2	|
|1	|1		|0.1	|
|1	|2		|0.1	|
|2	|1		|0.1	|
|2	|2		|0.1	|
|3	|3		|0.4	|
|	|		|		|
| 	|Soma	|1,0	|

|x	|y		|P(x,y)	|xy P(x,Y)	|x P(x)	|y P(y)	|x^2 P(x)	|
|-	|-		|-		|-			|-		|-		|-			|
|0	|0		|0.2	|0			|0		|0		|0			|
|1	|1		|0.1	|0.1		|0.1	|0.1	|0.1		|
|1	|2		|0.1	|0.2		|0.1	|0.2	|0.1		|
|2	|1		|0.1	|0.2		|0.2	|0.1	|0.4		|
|2	|2		|0.1	|0.4		|0.2	|0.2	|0.4		|
|3	|3		|0.4	|3.6		|1.2	|1.2	|3.6		|
|	|		|		|			|		|		|			|
|	|Soma	|1.0	|4.5		|1.8	|1.8	|4.6		|

### Exemplo cÃ¡culo de ğœŒ_XY

- Var(X) = E[X^2] - E[X]^2 = 4.6 - 3.24 = 1.36
- Var(Y) = Var(X)

- Cov(X,Y) = E[XY] - E[X] * E[Y] = 4.5 - 1.8 * 1.8 = 1.26

Finalmente:

ğœŒ_XY = Cov(X,Y) / (ğœ_X * ğœ_Y) = 1.25 / (sqr(1.36) * sqr(1.36)) = 0.926


